{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "made by weiyw @ 2019-4-10\n",
    "GAN\n",
    "'''\n",
    "## network workflow\n",
    "import os\n",
    "import keras\n",
    "import segyio\n",
    "import datetime\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D,Conv2DTranspose\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    merge,\n",
    "    Dense,\n",
    "    Lambda,\n",
    "    Reshape,\n",
    "    Dropout,\n",
    "    Concatenate\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDataloader():\n",
    "    def __init__(self, data_path, nt, nr, nph):\n",
    "        self.data_path = data_path\n",
    "        self.nt = nt\n",
    "        self.nr = nr\n",
    "        self.nph = nph\n",
    "\n",
    "\n",
    "    def normaliza(self, datain):\n",
    "        for batch_ii in range( datain.shape[0] ):\n",
    "            for iterm_ii in range( datain.shape[-1] ):\n",
    "                target = datain[batch_ii, :, :, iterm_ii]\n",
    "                mind = target.min() #datain[batch_ii, :, :, iterm_ii].min()\n",
    "                maxd = target.max() #datain[batch_ii, :, :, iterm_ii].max()      \n",
    "                for ii in range(target.shape[0]):\n",
    "                    for jj in range(target.shape[1]):\n",
    "                        if target[ii, jj] > 0: \n",
    "                            target[ii,jj] = target[ii,jj] / maxd\n",
    "                        if target[ii, jj] < 0:\n",
    "                            target[ii,jj] = target[ii,jj] / ( 0 - mind )\n",
    "                datain[batch_ii, :, :, iterm_ii] = target\n",
    "    #     return datain\n",
    "    \n",
    "    def load_batch(self, batch_size=1, is_testing=False, ratio=0.5):        \n",
    "        self.n_batches = int( 151 / batch_size * ratio ) #int( len(path) / batch_size * ratio )\n",
    "        x_data = np.empty((batch_size, self.nt, self.nr, self.nph)) ## b-2001-467-4, acc\n",
    "        y_data = np.empty((batch_size, self.nt, self.nr, 1)) ## b-2001-467-1, div, curl\n",
    "#         i = 0\n",
    "        with segyio.open(self.data_path,'r',ignore_geometry=True) as segyfile: \n",
    "            segyfile.mmap()\n",
    "#             while True:\n",
    "            for i in range(self.n_batches):\n",
    "#                 if (i + 1) * batch_size > 151 * ratio:\n",
    "#                     i = 0\n",
    "#                     break\n",
    "                for batch_i in range(batch_size):\n",
    "                    for nr_i in range(self.nr):\n",
    "#                     with segyio.open(self.data_path,'r',ignore_geometry=True) as segyfile:      \n",
    "                        y_data[batch_i,:,nr_i,0] = \\\n",
    "                        segyfile.trace[i*batch_size*4*self.nr + batch_i * 4 * self.nr + nr_i * 4 + 0]\n",
    "                        x_data[batch_i,:,nr_i,0] = \\\n",
    "                        segyfile.trace[i*batch_size*4*self.nr + batch_i * 4 * self.nr + nr_i * 4 + 1]\n",
    "                        x_data[batch_i,:,nr_i,1] = \\\n",
    "                        segyfile.trace[i*batch_size*4*self.nr + batch_i * 4 * self.nr + nr_i * 4 + 2]\n",
    "                        x_data[batch_i,:,nr_i,2] = \\\n",
    "                        segyfile.trace[i*batch_size*4*self.nr + batch_i * 4 * self.nr + nr_i * 4 + 3]                 \n",
    "                self.normaliza(x_data)\n",
    "                self.normaliza(y_data)\n",
    "                yield x_data, y_data\n",
    "#                 i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vspGAN():\n",
    "    def __init__(self):\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "        self.nt = 2001\n",
    "        self.nr = 467\n",
    "        self.nph = 3\n",
    "        self.nop = 1\n",
    "        \n",
    "        self.disc_patch = (int(self.nt / 2**4)+1, int(self.nr / 2**4)+1, 1)\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',optimizer=optimizer,metrics=['accuracy'])\n",
    "        \n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        data_B = Input( shape=( self.nt, self.nr, self.nph) )\n",
    "        data_A = Input( shape=( self.nt, self.nr, self.nop) )\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(data_B)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        valid = self.discriminator([data_B, fake_A])\n",
    "\n",
    "        self.combined = Model(inputs=[data_B, data_A], outputs=[fake_A,valid])\n",
    "        self.combined.compile(loss=['mse', 'mae'],loss_weights=[1, 100],optimizer=optimizer) \n",
    "        \n",
    "    def build_generator(self):  \n",
    "        '''\n",
    "        U-net generator\n",
    "        '''\n",
    "        def cutLayer(xx, target):\n",
    "            return xx[:, 0:int(target.shape[1]),0:int(target.shape[2]),0:int(target.shape[3])]\n",
    "    \n",
    "        def conv2d(layer_input, filters, f_size=(4,4), s_size=(2,2), bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            xx = Conv2D(filters=filters, kernel_size=f_size, strides=s_size, padding='same', \n",
    "                        data_format='channels_last', dilation_rate=(1, 1), activation=None, \n",
    "                        use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros', \n",
    "                        kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "                        kernel_constraint=None, bias_constraint=None)(layer_input)\n",
    "            if bn:\n",
    "                xx = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                           beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                           moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                           beta_constraint=None, gamma_constraint=None)(xx)\n",
    "            return xx\n",
    "        \n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=(4,4),s_size=(2,2), dropout_rate=0, if_skip=True,\n",
    "                     if_last=False):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            xx = Conv2DTranspose(filters=filters, kernel_size=f_size, strides=s_size, padding='same', output_padding=None, \n",
    "                         data_format='channels_last', dilation_rate=(1, 1), activation=None, use_bias=True, \n",
    "                         kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "                         kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                         kernel_constraint=None, bias_constraint=None)(layer_input)\n",
    "            if if_last:\n",
    "                xx = Conv2DTranspose(filters=filters, kernel_size=f_size, strides=s_size, padding='same', output_padding=None, \n",
    "                             data_format='channels_last', dilation_rate=(1, 1), activation='tanh', use_bias=True, \n",
    "                             kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "                             kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                             kernel_constraint=None, bias_constraint=None)(layer_input)\n",
    "                \n",
    "            if if_skip and xx.shape != skip_input.shape:\n",
    "                xx = Lambda(cutLayer, arguments={'target':(skip_input)})(xx)#(xx,skip_input)\n",
    "                \n",
    "            if dropout_rate:   \n",
    "                xx = Dropout(dropout_rate)(xx)\n",
    "                \n",
    "            if not if_last:\n",
    "                xx = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                           beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                           moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                           beta_constraint=None, gamma_constraint=None)(xx)\n",
    "                xx = Concatenate()([xx, skip_input])\n",
    "            return xx\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input( shape=( self.nt, self.nr, self.nph) )#shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*8)\n",
    "        d3 = conv2d(d2, self.gf*16)\n",
    "        d4 = conv2d(d2, self.gf*16)\n",
    "\n",
    "        # Upsampling\n",
    "    \n",
    "        u1 = deconv2d(d4, d3, self.gf*16)\n",
    "        u2 = deconv2d(u1, d2, self.gf*16)\n",
    "        u3 = deconv2d(u2, d1, self.gf*8)\n",
    "        u4 = deconv2d(u3, d0, self.nop, if_last=True)\n",
    "\n",
    "        return Model(inputs = d0, outputs = u4)\n",
    "    \n",
    "    \n",
    "    def build_discriminator(self):  \n",
    "        '''\n",
    "        U-net discriminator\n",
    "        ''' \n",
    "        def d_layer(layer_input, filters, f_size=(4,4),s_size=(2,2), bn=True):\n",
    "            \n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=s_size, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "        data_B = Input( shape=( self.nt, self.nr, self.nph) )\n",
    "        data_A = Input( shape=( self.nt, self.nr, self.nop) )\n",
    "        \n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_data = Concatenate(axis=-1)([data_B, data_A])\n",
    "\n",
    "        d1 = d_layer(combined_data, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([data_B, data_A], validity)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3; batch_size = 3; sample_interval = 2; ratio=0.5\n",
    "\n",
    "data_path = '/home/wyw/data/SEAM_I_walkaway_vsp_s23900/SEAM_Well1VSP_Shots23900.sgy'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "out_name = 'test_GAN'\n",
    "nt = 2001; nr = 467; ns = 151; nph = 3; nop = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = parameters().parse()\n",
    "GANmodel = vspGAN()\n",
    "\n",
    "D = GANmodel.discriminator\n",
    "G = GANmodel.generator\n",
    "C = GANmodel.combined\n",
    "# my_data_loader = GANDataloader(opt.data_path, opt.nt, opt.nr, opt.nph)\n",
    "my_data_loader = GANDataloader(data_path, nt, nr, nph)\n",
    "\n",
    "# filepath = os.path.join(save_dir,'{}.best.h5'.format(out_name))\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "# checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "milestones00\n",
      "milestones0\n",
      "milestones1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyw/anaconda3/envs/tf-keras/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 1/25] [D loss: 24.313932, acc:  23%] [G loss: 125.235130] time: 0:00:46.062967\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 2/25] [D loss: 10.006067, acc:  12%] [G loss: 253.147614] time: 0:01:03.458822\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 3/25] [D loss: 2.598898, acc:  20%] [G loss: 202.376373] time: 0:01:20.953811\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 4/25] [D loss: 5.896624, acc:  13%] [G loss: 151.195312] time: 0:01:38.392088\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 5/25] [D loss: 2.363771, acc:  33%] [G loss: 151.043213] time: 0:01:55.823097\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 6/25] [D loss: 5.116420, acc:  13%] [G loss: 256.153717] time: 0:02:13.201690\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 7/25] [D loss: 1.751547, acc:  21%] [G loss: 111.867828] time: 0:02:30.580753\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 8/25] [D loss: 2.045421, acc:  27%] [G loss: 116.315536] time: 0:02:48.062943\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 9/25] [D loss: 2.032746, acc:  20%] [G loss: 102.113182] time: 0:03:05.747204\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 10/25] [D loss: 1.294473, acc:  35%] [G loss: 102.178154] time: 0:03:23.186889\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 11/25] [D loss: 1.142727, acc:  33%] [G loss: 102.784607] time: 0:03:40.912945\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 12/25] [D loss: 1.112106, acc:  34%] [G loss: 100.452919] time: 0:03:58.674543\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 13/25] [D loss: 1.167097, acc:  34%] [G loss: 99.373985] time: 0:04:16.418860\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 14/25] [D loss: 1.552329, acc:  35%] [G loss: 103.353577] time: 0:04:34.117626\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 15/25] [D loss: 2.939933, acc:  27%] [G loss: 100.396179] time: 0:04:51.954373\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 16/25] [D loss: 2.552664, acc:  36%] [G loss: 97.926682] time: 0:05:09.884732\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 17/25] [D loss: 1.132534, acc:  35%] [G loss: 94.670815] time: 0:05:27.477105\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 18/25] [D loss: 1.097304, acc:  36%] [G loss: 94.091034] time: 0:05:45.149071\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 19/25] [D loss: 0.925491, acc:  37%] [G loss: 93.755798] time: 0:06:02.967466\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 20/25] [D loss: 0.939017, acc:  36%] [G loss: 91.755920] time: 0:06:20.910993\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 21/25] [D loss: 0.885131, acc:  37%] [G loss: 90.425545] time: 0:06:38.464530\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 22/25] [D loss: 0.911473, acc:  38%] [G loss: 90.008904] time: 0:06:56.313621\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 23/25] [D loss: 0.860903, acc:  38%] [G loss: 88.863762] time: 0:07:14.144701\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 24/25] [D loss: 0.837865, acc:  38%] [G loss: 87.558601] time: 0:07:32.000127\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 1/3] [Batch 25/25] [D loss: 0.801322, acc:  39%] [G loss: 86.448051] time: 0:07:49.731128\n",
      "model saved\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 1/25] [D loss: 0.755726, acc:  41%] [G loss: 84.759979] time: 0:08:08.033551\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 2/25] [D loss: 0.754275, acc:  41%] [G loss: 84.041618] time: 0:08:25.276650\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 3/25] [D loss: 0.726563, acc:  41%] [G loss: 83.267288] time: 0:08:42.830830\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 4/25] [D loss: 0.742841, acc:  40%] [G loss: 85.344101] time: 0:09:00.256647\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 5/25] [D loss: 0.740088, acc:  40%] [G loss: 83.101799] time: 0:09:17.780709\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 6/25] [D loss: 0.750228, acc:  41%] [G loss: 84.892502] time: 0:09:36.173905\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 7/25] [D loss: 0.767700, acc:  32%] [G loss: 81.381622] time: 0:09:54.796619\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 8/25] [D loss: 0.782495, acc:  38%] [G loss: 81.042084] time: 0:10:13.657308\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 9/25] [D loss: 0.798065, acc:  30%] [G loss: 82.741219] time: 0:10:32.499345\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 10/25] [D loss: 0.843440, acc:  36%] [G loss: 82.851570] time: 0:10:51.236256\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 11/25] [D loss: 0.913011, acc:  43%] [G loss: 83.954079] time: 0:11:09.730533\n",
      "milestones1\n",
      "milestones2\n",
      "milestones3\n",
      "[Epoch 2/3] [Batch 12/25] [D loss: 0.926799, acc:  28%] [G loss: 79.776329] time: 0:11:28.743441\n"
     ]
    }
   ],
   "source": [
    "disc_patch = GANmodel.disc_patch\n",
    "# disc_patch = (126,30,1)\n",
    "valid = np.ones((batch_size,) + disc_patch )\n",
    "fake = np.zeros((batch_size,) + disc_patch ) \n",
    "print(\"milestones00\")\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"milestones0\")\n",
    "for epoch in range(epochs):\n",
    "    for batch_i, (data_B, data_A) in enumerate(\n",
    "        my_data_loader.load_batch(batch_size=batch_size, is_testing=False, ratio=ratio)):\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        print(\"milestones1\")\n",
    "        # Condition on B and generate a translated version\n",
    "        fake_A = G.predict(data_B)\n",
    "        \n",
    "        # Train the discriminators (original images = real / generated = Fake)\n",
    "#         D.trainable = True#False\n",
    "        d_loss_real = D.train_on_batch([data_B, data_A], valid)\n",
    "        d_loss_fake = D.train_on_batch([data_B, fake_A], fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        print(\"milestones2\")\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        # Train the generators\n",
    "        g_loss = C.train_on_batch([data_B, data_A], [data_A, valid])\n",
    "        print(\"milestones3\")\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        # Plot the progress\n",
    "        print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % \n",
    "               (epoch+1, epochs, batch_i+1, my_data_loader.n_batches,\n",
    "                d_loss[0], 100*d_loss[1], g_loss[0], elapsed_time))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "#         if batch_i % sample_interval == 0:\n",
    "#             self.sample_data(epoch, batch_i)\n",
    "    if epoch % sample_interval == 0:\n",
    "        this_name = out_name + \"-\" + str(epoch+1)\n",
    "        D.save_weights(os.path.join( save_dir, '{}-D.h5'.format(this_name)), overwrite=True)\n",
    "        G.save_weights(os.path.join( save_dir, '{}-C.h5'.format(this_name)), overwrite=True)\n",
    "        print(\"model saved\")\n",
    "#             C.save_weights(os.path.join( save_dir, '{}-C-final-best.h5'.format(out_name)), overwrite=True)\n",
    "print(\"training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (tf-keras)",
   "language": "python",
   "name": "tf-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## headings\n",
    "\"\"\"\n",
    "made by weiyw @ 2019-03-08\n",
    "made to use the deep Spatio-temporal Residual Network\n",
    "\"\"\"\n",
    "# from __future__ import print_function\n",
    "# from preprocessing import *\n",
    "# import numpy as np\n",
    "# np.random.seed(2333)  # for reproducibility\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "# import h5py\n",
    "import struct\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "# from keras.layers.convolutional import Conv3d\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    merge,\n",
    "    Dense,\n",
    "    Reshape\n",
    ")\n",
    "# from STResNet import stresnet\n",
    "import metrics as metrics\n",
    "\n",
    "# from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_min_nonzero_value(data_in, init_min = 1):\n",
    "    min_value = init_min\n",
    "    for bb in data_in:\n",
    "        for aa in bb:\n",
    "            if abs(aa) < min_value and aa != 0:\n",
    "                min_value = abs(aa)\n",
    "    return min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_bindata(data_path, data_index, data_phase, nt, nr):\n",
    "    # data_index start from 1\n",
    "    model_num = int( ( int(data_index) - 1 ) / 51) + 1\n",
    "    source_num = ( int(data_index) - 1 ) % 51 + 1\n",
    "    data_name = data_path + \"/model\" + str(model_num) + \"source\" + str(source_num) + data_phase + \".bin\"\n",
    "#     print(data_name)\n",
    "    \n",
    "    out_data = np.empty((nt,nr))\n",
    "    FA = open(data_name, \"rb\")\n",
    "    FA.seek(3232,0)\n",
    "    for tt in range(nt):\n",
    "        for rr in range(nr):\n",
    "            data = FA.read(4)\n",
    "            data_float = struct.unpack(\"f\", data)[0]\n",
    "            out_data[tt][rr] = data_float\n",
    "     #np.shape(out_data)   # (4000,301)\n",
    "#     cut_data = out_data[0:4000, 8:401]\n",
    "    return out_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "    def __init__(self, data_path, nt, nr, nph):\n",
    "        self.data_path = data_path\n",
    "        self.nt = nt\n",
    "        self.nr = nr\n",
    "        self.nph = nph\n",
    "    \n",
    "    \n",
    "    def readin_bin_data(self, data_name):\n",
    "        out_data = np.empty((self.nt, self.nr))\n",
    "        FA = open(data_name, \"rb\")\n",
    "        FA.seek(3232,0)\n",
    "        for tt in range(self.nt):\n",
    "            for rr in range(self.nr):\n",
    "                data = FA.read(4)\n",
    "                data_float = struct.unpack(\"f\", data)[0]\n",
    "                out_data[tt][rr] = data_float\n",
    "         #np.shape(out_data)   # (4000,301)\n",
    "    #     cut_data = out_data[0:4000, 8:401]\n",
    "        return out_data  \n",
    "    \n",
    "    \n",
    "#     def load_data(self, batch_size=1, is_testing=False):\n",
    "#         data_type = \"train\" if not is_testing else \"test\"\n",
    "# #         path = os.path.join(self.data_path, self.dataset_name)\n",
    "        \n",
    "#         return x_data, y_data\n",
    "    \n",
    "    \n",
    "    def load_batch(self, batch_size=1, is_testing=False, ratio=0.5):\n",
    "        data_type = \"train\" if not is_testing else \"test\"\n",
    "#         path = glob('%s/%s/%s/*' % (self.data_path, self.dataset_name, data_type))\n",
    "        path = glob('%s/vz/*' % self.data_path)\n",
    "#         path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
    "#         path = os.path.join(self.data_path, self.dataset_name)\n",
    "        \n",
    "        self.n_batches = int( len(path) / batch_size * ratio )\n",
    "#         self.n_batches = 1\n",
    "        \n",
    "#         while True:\n",
    "        for i in range(self.n_batches):\n",
    "#             batch = path[ i * batch_size : (i+1) * batch_size]\n",
    "#             batch_num = [ i * batch_size : (i+1) * batch_size]\n",
    "#             x_data, y_data = [], [] ## 用list还是array？\n",
    "            x_data = np.empty((batch_size, self.nt, self.nr, self.nph)) ## b-4000-300-2, vz, vx\n",
    "            y_data = np.empty((batch_size, self.nt, self.nr, self.nph)) ## b-4000-300-2, div, curl\n",
    "            for data_index_local in range(batch_size): ## 0 ~ batchsize-1\n",
    "                data_index = i * batch_size + data_index_local + 1\n",
    "                model_num = int( ( int(data_index) - 1 ) / 51) + 1\n",
    "                source_num = ( int(data_index) - 1 ) % 51 + 1\n",
    "                if self.nph == 2:\n",
    "                    for data_phase in ['vz', 'vx']:\n",
    "                        p_flag = int(data_phase == 'vx')\n",
    "                        data_name = self.data_path + \"/\" + data_phase + \\\n",
    "                        \"/model\" + str(model_num) + \"source\" + str(source_num) + data_phase + \".bin\"\n",
    "#                         print(data_name)\n",
    "                        x_data[data_index_local, :, :, p_flag] = self.readin_bin_data(data_name)\n",
    "                    for data_phase in ['div', 'curl']:\n",
    "                        p_flag = int(data_phase == 'curl')\n",
    "                        data_name = self.data_path + \"/\" + data_phase + \\\n",
    "                        \"/model\" + str(model_num) + \"source\" + str(source_num) + data_phase + \".bin\"\n",
    "#                         print(data_name)\n",
    "                        y_data[data_index_local, :, :, p_flag] = self.readin_bin_data(data_name)\n",
    "                if self.nph == 1:\n",
    "                    data_phase = 'vz'\n",
    "                    data_name = self.data_path + \"/\" + data_phase + \\\n",
    "                    \"/model\" + str(model_num) + \"source\" + str(source_num) + data_phase + \".bin\"\n",
    "                    x_data[data_index_local, :, :, 0] = self.readin_bin_data(data_name)\n",
    "                    data_phase = 'div'\n",
    "                    data_name = self.data_path + \"/\" + data_phase + \\\n",
    "                    \"/model\" + str(model_num) + \"source\" + str(source_num) + data_phase + \".bin\"\n",
    "                    y_data[data_index_local, :, :, 0] = self.readin_bin_data(data_name)                    \n",
    "                yield x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def simpleNet(nt, nr, nph):\n",
    "    x_input = Input( shape=( nt, nr, nph) )##one_piece\n",
    "    conv1 = Convolution2D(\n",
    "        nb_filter=64, nb_row=3, nb_col=3, border_mode=\"same\", data_format=\"channels_last\")(x_input)\n",
    "    conv1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                               beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                               moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                               beta_constraint=None, gamma_constraint=None)(conv1)\n",
    "    conv2 = Convolution2D(\n",
    "        nb_filter=2, nb_row=3, nb_col=3, border_mode=\"same\", data_format=\"channels_last\")(conv1)\n",
    "    # conv1 = Conv3D(filters=51, kernel_size=(2,2,2), strides=(1,1,1), \n",
    "    #                             padding='same', data_format=\"channels_last\", dilation_rate=(1,1,1),\n",
    "    #                            activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    #                            bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "    #                            activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(x_input)\n",
    "    # conv1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "    #                            beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "    #                            moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "    #                            beta_constraint=None, gamma_constraint=None)(conv1)\n",
    "    # conv2 = Conv3D(filters=51, kernel_size=(2,2,2), strides=(1,1,1), \n",
    "    #                             padding='same', data_format=\"channels_last\", dilation_rate=(1,1,1),\n",
    "    #                            activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    #                            bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "    #                            activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(conv1)\n",
    "\n",
    "    # conv2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "    #                            beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "    #                            moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "    #                            beta_constraint=None, gamma_constraint=None)(conv2)\n",
    "\n",
    "    # conv3 = Conv3D(filters=51, kernel_size=(2,2,2), strides=(1,1,1), \n",
    "    #                             padding='same', data_format=\"channels_last\", dilation_rate=(1,1,1),\n",
    "    #                            activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    #                            bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "    #                            activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(conv2)\n",
    "    # conv3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "    #                            beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "    #                            moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "    #                            beta_constraint=None, gamma_constraint=None)(conv3)\n",
    "\n",
    "    # convLSTM_1 = keras.layers.ConvLSTM2D(filters=51, kernel_size=(2,2), strides=(1, 1), padding='valid', \n",
    "    #                                      data_format='channels_last', dilation_rate=(1, 1), activation='tanh', \n",
    "    #                                      recurrent_activation='hard_sigmoid', use_bias=True,\n",
    "    #                                      kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "    #                                      bias_initializer='zeros', unit_forget_bias=True, \n",
    "    #                                      kernel_regularizer=None, recurrent_regularizer=None, \n",
    "    #                                      bias_regularizer=None, activity_regularizer=None, \n",
    "    #                                      kernel_constraint=None, recurrent_constraint=None, \n",
    "    #                                      bias_constraint=None, return_sequences=True, \n",
    "    #                                      go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0)(conv3)\n",
    "\n",
    "\n",
    "    model = Model(inputs=x_input, outputs=conv2)\n",
    "    # model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyw/anaconda3/envs/tf-keras/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(data_format=\"channels_last\", kernel_size=(3, 3), filters=64, padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/wyw/anaconda3/envs/tf-keras/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(data_format=\"channels_last\", kernel_size=(3, 3), filters=2, padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/media/wywdisk/VSPdata/data/haveinvx/layer2_haveinvx\"\n",
    "\n",
    "nt = 4000     # time step\n",
    "nr = 400      # receiver\n",
    "ns = 51       # shot\n",
    "nmodel = 2    # batch\n",
    "nph = 2\n",
    "\n",
    "model = simpleNet(nt, nr, nph)\n",
    "my_data_loader = Dataloader(data_path, nt, nr, nph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ## check for input data\n",
    "# show_data = one_piece[:,:,0,0]\n",
    "# # show_data = this_data[0]\n",
    "# target = \"show_data\" #\"this_data[0]\"\n",
    "# print \"The shape of \", target, \"\\tis\", eval(target).shape\n",
    "# print \"The max:\\t\",  eval(target).max(), \"\\tThe min:\\t\",  eval(target).min()\n",
    "# print \"The mean:\\t\",  eval(target).mean(), \"\\tThe std:\\t\",  eval(target).std()\n",
    "# print \"The absolute minimal value is\\t\", get_min_nonzero_value(eval(target))\n",
    "# print \"===========================================================================\"\n",
    "# show_data = one_piece[:,:,1,0]\n",
    "# # show_data = this_data[1]\n",
    "# target = \"show_data\" #\"this_data[0]\"\n",
    "# print \"The shape of \", target, \"\\tis\", eval(target).shape\n",
    "# print \"The max:\\t\",  eval(target).max(), \"\\tThe min:\\t\",  eval(target).min()\n",
    "# print \"The mean:\\t\",  eval(target).mean(), \"\\tThe std:\\t\",  eval(target).std()\n",
    "# print \"The absolute minimal value is\\t\", get_min_nonzero_value(eval(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "## prepare data for input\n",
    "## 如何迭代输入数据？\n",
    "# this_data_3d = one_piece.reshape((nt*nph,nr,ns))\n",
    "# indata = tf.convert_to_tensor(this_data_3d)\n",
    "\n",
    "# this_data_5d = one_piece #.reshape((nt*nph,nr,ns))\n",
    "# indata = tf.convert_to_tensor(this_data_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training parameters\n",
    "TPepoches = 4\n",
    "TPbatch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(TPepoches):\n",
    "#     for bacth_ii, (x_data, y_data) in \\\n",
    "#     enumerate(my_data_loader.load_batch(batch_size=TPbatch_size, is_testing=False, ratio=0.001)):       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4000, 400, 2)\n",
      "(1, 4000, 400, 2)\n",
      "(1, 4000, 400, 2)\n",
      "(1, 4000, 400, 2)\n",
      "\n",
      "elapsed time (compiling model): 10.214 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## make single data\n",
    "ts = time.time()\n",
    "data_path = \"/media/wywdisk/VSPdata/data/haveinvx/layer2_haveinvx\"\n",
    "havebatch = True; data_phases = [\"vz\", \"vx\"];  nmodel = 1\n",
    "\n",
    "if havebatch:\n",
    "    ## prepare files\n",
    "    one_piece = np.empty((nmodel,nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 1 ## change here !!!\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        one_piece[0,:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "    valid_piece = np.empty((nmodel,nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 2\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        valid_piece[0,:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "    test_piece = np.empty((nmodel,nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 3\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        test_piece[0,:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "else:    \n",
    "    ## prepare files\n",
    "    one_piece = np.empty((nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 1\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        one_piece[:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "    valid_piece = np.empty((nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 2\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        valid_piece[:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "    test_piece = np.empty((nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 3\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        test_piece[:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "\n",
    "if havebatch :\n",
    "    ## prepare files\n",
    "    one_piece_y = np.empty((nmodel,nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 1\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        one_piece_y[0,:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "    ## prepare files\n",
    "    valid_piece_y = np.empty((nmodel,nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 2\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        valid_piece_y[0,:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "    ## prepare files\n",
    "    test_piece_y = np.empty((nmodel,nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 10    # for iteration [1, totalNum]\n",
    "    data_index = 3\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        test_piece_y[0,:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "else:\n",
    "    ## prepare files\n",
    "    one_piece_y = np.empty((nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 1\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        one_piece_y[:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "    ## prepare files\n",
    "    valid_piece_y = np.empty((nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 1    # for iteration [1, totalNum]\n",
    "    data_index = 2\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        valid_piece_y[:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "    ## prepare files\n",
    "    test_piece_y = np.empty((nt, nr, nph)) # 4000, 300, 2, 51\n",
    "    # model_count = 10    # for iteration [1, totalNum]\n",
    "    data_index = 3\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        test_piece_y[:,:,p_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "        \n",
    "# print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n",
    "#         (time.time() - ts))\n",
    "\n",
    "# x_train, y_train = tf.convert_to_tensor(one_piece), tf.convert_to_tensor(one_piece_y)\n",
    "x_valid, y_valid = tf.convert_to_tensor(valid_piece), tf.convert_to_tensor(valid_piece_y)\n",
    "x_test, y_test = tf.convert_to_tensor(test_piece), tf.convert_to_tensor(test_piece_y)\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n",
    "        (time.time() - ts)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "## complete the training process\n",
    "# adam = Adam(lr=lr)\n",
    "# model.compile(loss='mse', optimizer=adam, metrics=[rmse])\n",
    "#  #   model.summary()\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "## https://keras.io/models/model/\n",
    "# '''\n",
    "# loss: mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_logarithmic_error,\n",
    "#         squared_hinge, categorical_hinge, logcosh, categorical_crossentropy, sparse_categorical_crossentropy,\n",
    "#         binary_crossentropy, kullback_leibler_divergence, poisson, cosine_proximity\n",
    "# optimizer: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "# metrics: ?\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "train_size = 1\n",
    "test_size = 1\n",
    "time_size = 4000\n",
    "nb_residual_unit = 0\n",
    "lr = 0\n",
    "# model_name = 'vsp_%s_model.{epoch:03d}.h5' % model_type\n",
    "hyperparams_name = 'train_size{}.test_size{}.time_size{}.resunit{}.lr{}'.format(\n",
    "        train_size, test_size, time_size, nb_residual_unit, lr)\n",
    "fname_param = os.path.join(save_dir,'{}.best.h5'.format(hyperparams_name))\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = fname_param#os.path.join(save_dir, fname_param)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_rmse', patience=2, mode='min')\n",
    "# model_checkpoint = ModelCheckpoint(\n",
    "#     fname_param, monitor='rmse', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n",
    "#         (time.time() - ts))\n",
    "\n",
    "# print('=' * 10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "callbacks = [checkpoint]#, lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(os.path.join('MODEL', '{}.best.h5'.format(hyperparams_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "4/4 [==============================] - 22s 5s/step - loss: 9.5217e-12 - acc: 0.5147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyw/anaconda3/envs/tf-keras/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4\n",
      "4/4 [==============================] - 18s 4s/step - loss: 3.1291e-06 - acc: 0.5003\n",
      "Epoch 3/4\n",
      "4/4 [==============================] - 18s 5s/step - loss: 2.2009e-06 - acc: 0.6035\n",
      "Epoch 4/4\n",
      "3/4 [=====================>........] - ETA: 4s - loss: 8.2012e-07 - acc: 0.3336"
     ]
    }
   ],
   "source": [
    "# for bacth_ii, (x_data, y_data) in enumerate(my_data_loader.load_batch(batch_size=TPbatch_size, is_testing=False, ratio=0.001)):\n",
    "# model.fit_generator(my_data_loader.load_batch(batch_size=TPbatch_size, is_testing=False, ratio=0.001),\\\n",
    "#               steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, \\\n",
    "#               validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, \\\n",
    "#               workers=1, use_multiprocessing=False, shuffle=False, initial_epoch=0)\n",
    "\n",
    "log = model.fit_generator(my_data_loader.load_batch(batch_size=TPbatch_size, is_testing=False, ratio=0.1),\\\n",
    "              steps_per_epoch=TPbatch_size, epochs=TPepoches, verbose=1, callbacks=callbacks, validation_data=None, \\\n",
    "              validation_steps=None, class_weight=None, max_queue_size=10, \\\n",
    "              workers=1, use_multiprocessing=False, shuffle=False, initial_epoch=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # print \"training model...\"\n",
    "# ts = time.time()\n",
    "\n",
    "# # model.fit(x_train, y_train,\n",
    "# #           batch_size=batch_size,\n",
    "# #           epochs=epochs,\n",
    "# #           validation_data=(x_valid, y_valid),\n",
    "# #           shuffle=True,\n",
    "# #           steps_per_epoch=None,\n",
    "# #           validation_steps=None,\n",
    "# #           callbacks=callbacks)\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=None,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(x_valid, y_valid),\n",
    "#           shuffle=True,\n",
    "#           steps_per_epoch=1,\n",
    "#           validation_steps=1,\n",
    "#           callbacks=callbacks)\n",
    "# print(\"\\nelapsed time (training cont): %.3f seconds\\n\" % (time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(fname_param)\n",
    "# from models import model_from_json\n",
    "\n",
    "json_string = model.to_json()\n",
    "open('model_json', 'w').write(json_string)\n",
    "#model = model_from_json(json_string)\n",
    "model.save_weights(os.path.join( save_dir, '{}.final.best.h5'.format(hyperparams_name)), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "ts = time.time()\n",
    "scores = model.evaluate(x_valid, y_valid, batch_size=None, steps=1, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "# print('Test score: %.6f rmse (norm): %.6f rmse (real): %.6f' %\n",
    "#         (score[0], score[1], score[1] * (mx - mn) / 2.))\n",
    "print(\"\\nelapsed time (eval cont): %.3f seconds\\n\" % (time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prdiction\n",
    "# y_predict = model.predict(x_test, batch_size=None, steps=1, verbose = 1)\n",
    "\n",
    "predict = model.predict_generator(my_data_loader.load_batch(batch_size=TPbatch_size, is_testing=True, ratio=0.1),\\\n",
    "                                  steps=1, max_queue_size=10, \\\n",
    "                                  workers=1, use_multiprocessing=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for predicted data\n",
    "a1 = y_predict\n",
    "print(\"Information of a1 :\")\n",
    "print(\"The shape of a1   :\",a1.shape)\n",
    "print(\"The range        : \", [a1.min(),a1.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot y_predict\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "lines = 1\n",
    "cols = 2\n",
    "forts = 12\n",
    "ss1 = 300\n",
    "ss2 = 400\n",
    "\n",
    "x_libs = [0,0, 500, 1000, 1500, 2000]#np.linspace(0,1500,4)#[0, 500, 1000, 1500]\n",
    "y_libs = np.linspace(0,2,9)#[0,0.4, 0.8, 1.2, 1.6, 2]\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10, 4), sharey=True)\n",
    "\n",
    "# obj = 'y_predict'\n",
    "# im = plt.imshow(eval(obj)[0,:,:,1], extent=[0, 512, 512,0])\n",
    "# plt.colorbar(im)\n",
    "\n",
    "# ax1 = plt.subplot(131)\n",
    "ax1 = plt.subplot(lines,cols,1)\n",
    "obj = 'y_predict'\n",
    "# print(obj, \": \", eval(obj).shape, \"\\t\", [eval(obj).min(), eval(obj).max()] )\n",
    "im = plt.imshow(eval(obj)[0,:,:,0], extent=[0, ss1, ss2,0])\n",
    "plt.colorbar(im)\n",
    "# plt.title(\"The elastic z component of VSP data\", fontsize=forts);\n",
    "plt.title(\"predicted P-wave\", fontsize=forts);\n",
    "plt.xlabel('depth(m)', fontsize=forts);\n",
    "plt.ylabel('time(s)', fontsize=forts);\n",
    "# ax1 = fig.add_subplot(131)\n",
    "ax1.xaxis.set_major_locator(MultipleLocator(75))\n",
    "ax1.xaxis.set(ticklabels=x_libs)#[0,300,600, 900, 1200, 1500]);\n",
    "ax1.yaxis.set(ticklabels=y_libs)#[0,0.4, 0.8, 1.2, 1.6, 2]);\n",
    "plt.setp(ax1.get_xticklabels(), fontsize=10);\n",
    "# plt.tight_layout()\n",
    "\n",
    "# ax1 = plt.subplot(131)\n",
    "ax2 = plt.subplot(lines,cols,2)\n",
    "obj = 'y_predict'\n",
    "# print(obj, \": \", eval(obj).shape, \"\\t\", [eval(obj).min(), eval(obj).max()] )\n",
    "im = plt.imshow(eval(obj)[0,:,:,1], extent=[0, ss1, ss2,0])\n",
    "plt.colorbar(im)\n",
    "# plt.title(\"The elastic z component of VSP data\", fontsize=forts);\n",
    "plt.title(\"predicted S-wave\", fontsize=forts);\n",
    "plt.xlabel('depth(m)', fontsize=forts);\n",
    "plt.ylabel('time(s)', fontsize=forts);\n",
    "# ax1 = fig.add_subplot(131)\n",
    "ax2.xaxis.set_major_locator(MultipleLocator(75))\n",
    "ax2.xaxis.set(ticklabels=x_libs)#[0,300,600, 900, 1200, 1500]);\n",
    "ax2.yaxis.set(ticklabels=y_libs)#[0,0.4, 0.8, 1.2, 1.6, 2]);\n",
    "plt.setp(ax2.get_xticklabels(), fontsize=10);\n",
    "# plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (tf-keras)",
   "language": "python",
   "name": "tf-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

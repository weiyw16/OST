{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "## headings\n",
    "\"\"\"\n",
    "made by weiyw @ 2019-03-08\n",
    "made to use the deep Spatio-temporal Residual Network\n",
    "\"\"\"\n",
    "# from __future__ import print_function\n",
    "# from preprocessing import *\n",
    "# import numpy as np\n",
    "# np.random.seed(2333)  # for reproducibility\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import struct\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "# from STResNet import stresnet\n",
    "import metrics as metrics\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_min_nonzero_value(data_in, init_min = 1):\n",
    "    min_value = init_min\n",
    "    for bb in data_in:\n",
    "        for aa in bb:\n",
    "            if abs(aa) < min_value and aa != 0:\n",
    "                min_value = abs(aa)\n",
    "    return min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# def prepare_data(filepath, all_times, data_step, data_stride):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for i in range(all_times):\n",
    "#         count_X = range(i*data_stride, i*data_stride + data_step)\n",
    "#         for j in count_X:\n",
    "#             img = Image.open(filepath + \"/\" + str(j+1)+\".png\")\n",
    "#             data = np.resize(np.asarray(img), (330, 580, 4))#np.asarray(img)\n",
    "#             X.append(data[:,:,0])\n",
    "#         count_y = i*data_stride + data_step\n",
    "#         img = Image.open(filepath + \"/\" + str(count_y+1)+\".png\")\n",
    "#         data = np.resize(np.asarray(img), (330, 580, 4))#np.asarray(img)\n",
    "#         y.append(data[:,:,0])\n",
    "#     X = np.array(X, dtype='float32')\n",
    "#     y = np.array(y, dtype='float32')\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_bindata(data_path, data_index, data_phase, nt, nr):\n",
    "    # data_index start from 1\n",
    "    model_num = int( ( int(data_index) - 1 ) / 51) + 1\n",
    "    source_num = ( int(data_index) - 1 ) % 51 + 1\n",
    "    data_name = data_path + \"/model\" + str(model_num) + \"source\" + str(source_num) + data_phase + \".bin\"\n",
    "#     print(data_name)\n",
    "    \n",
    "    out_data = np.empty((nt,nr))\n",
    "    FA = open(data_name, \"rb\")\n",
    "    FA.seek(3232,0)\n",
    "    for tt in range(nt):\n",
    "        for rr in range(nr):\n",
    "            data = FA.read(4)\n",
    "            data_float = struct.unpack(\"f\", data)[0]\n",
    "            out_data[tt][rr] = data_float\n",
    "     #np.shape(out_data)   # (4000,301)\n",
    "#     cut_data = out_data[0:4000, 8:401]\n",
    "    return out_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## loadin data\n",
    "ts = time.time()\n",
    "# data_path = \"./mydata/first_test/\"\n",
    "# data_path = \"/media/wywdisk/VSPdata/data/layer2_1-90_rightPML\"\n",
    "data_path = \"/media/wywdisk/VSPdata/data/haveinvx/layer2_haveinvx\"\n",
    "\n",
    "## count the total number in one data_path\n",
    "# datapath_list = os.listdir( os.path.join(data_path,'vz') )\n",
    "# datapath_list.sort()\n",
    "# totalNum = len(datapath_list)\n",
    "# print(totalNum)\n",
    "\n",
    "data_phases = [\"vz\", \"vx\"]\n",
    "nph = len(data_phases)     # phase number\n",
    "nt = 4000     # time step\n",
    "nr = 300      # receiver\n",
    "ns = 51       # shot\n",
    "nmodel = 1    # batch\n",
    "'''\n",
    "3D conv, (batch, 4000, 300, 2, 51) channels_last\n",
    "'''\n",
    "## prepare files\n",
    "one_piece = np.empty((1,nt, nr, nph, ns)) # 4000, 300, 2, 51\n",
    "model_count = 1    # for iteration [1, totalNum]\n",
    "for data_index in range( (model_count - 1) * 51 + 1, model_count * 51 + 1 ):\n",
    "    s_flag = ( int(data_index) - 1 ) % 51      # [0-51)\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        one_piece[0,:,:,p_flag,s_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "## prepare files\n",
    "valid_piece = np.empty((1, nt, nr, nph, ns)) # 4000, 300, 2, 51\n",
    "model_count = 2    # for iteration [1, totalNum]\n",
    "for data_index in range( (model_count - 1) * 51 + 1, model_count * 51 + 1 ):\n",
    "    s_flag = ( int(data_index) - 1 ) % 51      # [0-51)\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        valid_piece[0,:,:,p_flag,s_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "## prepare files\n",
    "test_piece = np.empty((1, nt, nr, nph, ns)) # 4000, 300, 2, 51\n",
    "model_count = 3    # for iteration [1, totalNum]\n",
    "for data_index in range( (model_count - 1) * 51 + 1, model_count * 51 + 1 ):\n",
    "    s_flag = ( int(data_index) - 1 ) % 51      # [0-51)\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'vx')      # vz=0, vx=1\n",
    "        test_piece[0,:,:,p_flag,s_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n",
    "        (time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "elapsed time (compiling model): 171.873 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## loadin data\n",
    "ts = time.time()\n",
    "# data_path = \"./mydata/first_test/\"\n",
    "# data_path = \"/media/wywdisk/VSPdata/data/layer2_1-90_rightPML\"\n",
    "data_path = \"/media/wywdisk/VSPdata/data/haveinvx/layer2_haveinvx\"\n",
    "\n",
    "## count the total number in one data_path\n",
    "# datapath_list = os.listdir( os.path.join(data_path,'vz') )\n",
    "# datapath_list.sort()\n",
    "# totalNum = len(datapath_list)\n",
    "# print(totalNum)\n",
    "\n",
    "data_phases = [\"div\", \"curl\"]\n",
    "nph = len(data_phases)     # phase number\n",
    "nt = 4000     # time step\n",
    "nr = 300      # receiver\n",
    "ns = 51       # shot\n",
    "nmodel = 1    # batch\n",
    "'''\n",
    "3D conv, (batch, 4000, 300, 2, 51) channels_last\n",
    "'''\n",
    "## prepare files\n",
    "one_piece_y = np.empty((nmodel,nt, nr, nph, ns)) # 4000, 300, 2, 51\n",
    "model_count = 1    # for iteration [1, totalNum]\n",
    "for data_index in range( (model_count - 1) * 51 + 1, model_count * 51 + 1 ):\n",
    "    s_flag = ( int(data_index) - 1 ) % 51      # [0-51)\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        one_piece_y[0,:,:,p_flag,s_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "## prepare files\n",
    "valid_piece_y = np.empty((nmodel, nt, nr, nph, ns)) # 4000, 300, 2, 51\n",
    "model_count = 2    # for iteration [1, totalNum]\n",
    "for data_index in range( (model_count - 1) * 51 + 1, model_count * 51 + 1 ):\n",
    "    s_flag = ( int(data_index) - 1 ) % 51      # [0-51)\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        valid_piece_y[0,:,:,p_flag,s_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "## prepare files\n",
    "test_piece_y = np.empty((nmodel, nt, nr, nph, ns)) # 4000, 300, 2, 51\n",
    "model_count = 3    # for iteration [1, totalNum]\n",
    "for data_index in range( (model_count - 1) * 51 + 1, model_count * 51 + 1 ):\n",
    "    s_flag = ( int(data_index) - 1 ) % 51      # [0-51)\n",
    "    for data_phase in data_phases:\n",
    "        p_flag = int(data_phase == 'curl')      # vz=0, vx=1\n",
    "        test_piece_y[0,:,:,p_flag,s_flag] = get_bindata(os.path.join(data_path, data_phase), data_index, data_phase, nt, nr) \n",
    "\n",
    "print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n",
    "        (time.time() - ts)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## check for input data\n",
    "show_data = one_piece[:,:,0,0]\n",
    "# show_data = this_data[0]\n",
    "target = \"show_data\" #\"this_data[0]\"\n",
    "print \"The shape of \", target, \"\\tis\", eval(target).shape\n",
    "print \"The max:\\t\",  eval(target).max(), \"\\tThe min:\\t\",  eval(target).min()\n",
    "print \"The mean:\\t\",  eval(target).mean(), \"\\tThe std:\\t\",  eval(target).std()\n",
    "print \"The absolute minimal value is\\t\", get_min_nonzero_value(eval(target))\n",
    "print \"===========================================================================\"\n",
    "show_data = one_piece[:,:,1,0]\n",
    "# show_data = this_data[1]\n",
    "target = \"show_data\" #\"this_data[0]\"\n",
    "print \"The shape of \", target, \"\\tis\", eval(target).shape\n",
    "print \"The max:\\t\",  eval(target).max(), \"\\tThe min:\\t\",  eval(target).min()\n",
    "print \"The mean:\\t\",  eval(target).mean(), \"\\tThe std:\\t\",  eval(target).std()\n",
    "print \"The absolute minimal value is\\t\", get_min_nonzero_value(eval(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data for input\n",
    "## 如何迭代输入数据？\n",
    "# this_data_3d = one_piece.reshape((nt*nph,nr,ns))\n",
    "# indata = tf.convert_to_tensor(this_data_3d)\n",
    "\n",
    "# this_data_5d = one_piece #.reshape((nt*nph,nr,ns))\n",
    "# indata = tf.convert_to_tensor(this_data_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4000, 300, 2, 51)\n",
      "(1, 4000, 300, 2, 51)\n",
      "(1, 4000, 300, 2, 51)\n",
      "(1, 4000, 300, 2, 51)\n",
      "(1, 4000, 300, 2, 51)\n",
      "(1, 4000, 300, 2, 51)\n",
      "\n",
      "elapsed time (compiling model): 48.957 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "x_train, y_train = tf.convert_to_tensor(one_piece), tf.convert_to_tensor(one_piece_y)\n",
    "x_valid, y_valid = tf.convert_to_tensor(valid_piece), tf.convert_to_tensor(valid_piece_y)\n",
    "x_test, y_test = tf.convert_to_tensor(test_piece), tf.convert_to_tensor(test_piece_y)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n",
    "        (time.time() - ts)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4000, 300, 2, 51)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 4000, 300, 2, 51)  20859     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4000, 300, 2, 51)  204       \n",
      "=================================================================\n",
      "Total params: 21,063\n",
      "Trainable params: 20,961\n",
      "Non-trainable params: 102\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## network workflow\n",
    "import keras\n",
    "from keras.models import Model\n",
    "# from keras.layers.convolutional import Convolution2D\n",
    "# from keras.layers.convolutional import Conv3d\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    merge,\n",
    "    Dense,\n",
    "    Reshape\n",
    ")\n",
    "x_input = Input( shape=(nt, nr, nph, ns) )##one_piece\n",
    "# conv1 = Convolution2D(\n",
    "#     nb_filter=64, nb_row=3, nb_col=3, border_mode=\"same\", data_format=\"channels_last\")(x_input)\n",
    "conv1 = Conv3D(filters=51, kernel_size=(2,2,2), strides=(1,1,1), \n",
    "                            padding='same', data_format=\"channels_last\", dilation_rate=(1,1,1),\n",
    "                           activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                           bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                           activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(x_input)\n",
    "conv1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                           beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                           moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                           beta_constraint=None, gamma_constraint=None)(conv1)\n",
    "# conv1 = Conv3D(filters=64, kernel_size=(2,2,2), strides=(1,1,1), \n",
    "#                             padding='valid', data_format=\"channels_last\", dilation_rate=(1,1,1),\n",
    "#                            activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#                            bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#                            activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(x_input)\n",
    "\n",
    "conv2 = Conv3D(filters=51, kernel_size=(2,2,2), strides=(1,1,1), \n",
    "                            padding='same', data_format=\"channels_last\", dilation_rate=(1,1,1),\n",
    "                           activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                           bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                           activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(conv1)\n",
    "\n",
    "conv2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                           beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                           moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                           beta_constraint=None, gamma_constraint=None)(conv2)\n",
    "\n",
    "conv3 = Conv3D(filters=51, kernel_size=(2,2,2), strides=(1,1,1), \n",
    "                            padding='same', data_format=\"channels_last\", dilation_rate=(1,1,1),\n",
    "                           activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                           bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                           activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(conv2)\n",
    "conv3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                           beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                           moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                           beta_constraint=None, gamma_constraint=None)(conv3)\n",
    "\n",
    "convLSTM_1 = keras.layers.ConvLSTM2D(filters=51, kernel_size=(2,2), strides=(1, 1), padding='valid', \n",
    "                                     data_format='channels_last', dilation_rate=(1, 1), activation='tanh', \n",
    "                                     recurrent_activation='hard_sigmoid', use_bias=True,\n",
    "                                     kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "                                     bias_initializer='zeros', unit_forget_bias=True, \n",
    "                                     kernel_regularizer=None, recurrent_regularizer=None, \n",
    "                                     bias_regularizer=None, activity_regularizer=None, \n",
    "                                     kernel_constraint=None, recurrent_constraint=None, \n",
    "                                     bias_constraint=None, return_sequences=True, \n",
    "                                     go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0)(conv3)\n",
    "\n",
    "# model = Model(inputs=x_input, outputs=convLSTM_1)\n",
    "model = Model(inputs=x_input, outputs=conv1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ## model-class\n",
    "# '''\n",
    "# STrans-ResNet: Deep Spatial transform with time Residual Networks\n",
    "# modified by weiyw @ 2019-04-02 from ST-ResNet\n",
    "# '''\n",
    "# from __future__ import print_function\n",
    "# from keras.layers import (\n",
    "#     Input,\n",
    "#     Activation,\n",
    "#     merge,\n",
    "#     Dense,\n",
    "#     Reshape\n",
    "# )\n",
    "# from keras.layers.convolutional import Convolution2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.models import Model\n",
    "# #from keras.utils.visualize_util import plot\n",
    "\n",
    "\n",
    "# def _shortcut(input, residual):\n",
    "#     return merge([input, residual], mode='sum')\n",
    "\n",
    "\n",
    "# def _bn_relu_conv(nb_filter, nb_row, nb_col, subsample=(1, 1), bn=False):\n",
    "#     def f(input):\n",
    "#         if bn:\n",
    "#             input = BatchNormalization(mode=0, axis=1)(input) ## attention!!\n",
    "#         activation = Activation('relu')(input)\n",
    "#         return Convolution2D(nb_filter=nb_filter, nb_row=nb_row, nb_col=nb_col, subsample=subsample, border_mode=\"same\")(activation)\n",
    "#     return f\n",
    "\n",
    "\n",
    "# def _residual_unit(nb_filter, init_subsample=(1, 1)):\n",
    "#     def f(input):\n",
    "#         residual = _bn_relu_conv(nb_filter, 3, 3)(input)\n",
    "#         residual = _bn_relu_conv(nb_filter, 3, 3)(residual)\n",
    "#         return _shortcut(input, residual)\n",
    "#     return f\n",
    "\n",
    "\n",
    "# def ResUnits(residual_unit, nb_filter, repetations=1):\n",
    "#     def f(input):\n",
    "#         for i in range(repetations):\n",
    "#             init_subsample = (1, 1)\n",
    "#             input = residual_unit(nb_filter=nb_filter,\n",
    "#                                   init_subsample=init_subsample)(input)\n",
    "#         return input\n",
    "#     return f\n",
    "\n",
    "\n",
    "# # def stresnet(c_conf=(3, 2, 32, 32), p_conf=(3, 2, 32, 32), t_conf=(3, 2, 32, 32), external_dim=8, nb_residual_unit=3):\n",
    "# def stresnet(total_conf=(4000, 2, 300, 51), external_dim=None, nb_residual_unit=3):\n",
    "#     '''\n",
    "#     C - Temporal Closeness\n",
    "#     P - Period\n",
    "#     T - Trend\n",
    "#     conf = (len_seq, nb_flow, map_height, map_width)\n",
    "#     external_dim\n",
    "#     '''\n",
    "#     # main input\n",
    "#     main_inputs = []\n",
    "#     outputs = []\n",
    "#     for conf in [total_conf] :#[c_conf, p_conf, t_conf]:\n",
    "#         if conf is not None:\n",
    "#             len_seq, nb_flow, map_height, map_width = conf\n",
    "#             input = Input(shape=(nb_flow * len_seq, map_height, map_width))\n",
    "#             main_inputs.append(input)\n",
    "#             # Conv1\n",
    "# #             conv1 = Convolution2D(\n",
    "# #                 nb_filter=64, nb_row=2, nb_col=2, border_mode=\"same\")(input)\n",
    "#             conv1 = Convolution2D(\n",
    "#                 nb_filter=64, nb_row=3, nb_col=3, border_mode=\"same\")(input)\n",
    "#             # [nb_residual_unit] Residual Units\n",
    "#             residual_output = ResUnits(_residual_unit, nb_filter=64,\n",
    "#                               repetations=nb_residual_unit)(conv1)\n",
    "#             # Conv2\n",
    "#             activation = Activation('relu')(residual_output)\n",
    "#             conv2 = Convolution2D(\n",
    "#                 nb_filter=nb_flow, nb_row=3, nb_col=3, border_mode=\"same\")(activation)\n",
    "# #             conv2 = Convolution2D(\n",
    "# #                 nb_filter=nb_flow, nb_row=2, nb_col=2, border_mode=\"same\")(activation)\n",
    "\n",
    "#             outputs.append(conv2)\n",
    "\n",
    "#     # parameter-matrix-based fusion\n",
    "#     if len(outputs) == 1:\n",
    "#         main_output = outputs[0]\n",
    "#     else:\n",
    "#         from .iLayer import iLayer\n",
    "#         new_outputs = []\n",
    "#         for output in outputs:\n",
    "#             new_outputs.append(iLayer()(output))\n",
    "#         main_output = merge(new_outputs, mode='sum')\n",
    "\n",
    "#     # fusing with external component\n",
    "#     if external_dim != None and external_dim > 0:\n",
    "#         # external input\n",
    "#         external_input = Input(shape=(external_dim,))\n",
    "#         main_inputs.append(external_input)\n",
    "#         embedding = Dense(output_dim=10)(external_input)\n",
    "#         embedding = Activation('relu')(embedding)\n",
    "#         h1 = Dense(output_dim=nb_flow * map_height * map_width)(embedding)\n",
    "#         activation = Activation('relu')(h1)\n",
    "#         external_output = Reshape((nb_flow, map_height, map_width))(activation)\n",
    "#         main_output = merge([main_output, external_output], mode='sum')\n",
    "#     else:\n",
    "#         print('external_dim:', external_dim)\n",
    "\n",
    "#     main_output = Activation('tanh')(main_output)\n",
    "#     model = Model(input=main_inputs, output=main_output)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "# # #     model = stresnet(external_dim=28, nb_residual_unit=12)\n",
    "# #     model = stresnet(external_dim=None, nb_residual_unit=8)\n",
    "# #     #plot(model, to_file='ST-ResNet.png', show_shapes=True)\n",
    "# #     model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training parameters\n",
    "batch_size = nmodel#1  # orig paper trained all networks with batch_size=128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learning rate: ', 0.001)\n"
     ]
    }
   ],
   "source": [
    "## complete the training process\n",
    "# adam = Adam(lr=lr)\n",
    "# model.compile(loss='mse', optimizer=adam, metrics=[rmse])\n",
    "#  #   model.summary()\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "## https://keras.io/models/model/\n",
    "# '''\n",
    "# loss: mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_logarithmic_error,\n",
    "#         squared_hinge, categorical_hinge, logcosh, categorical_crossentropy, sparse_categorical_crossentropy,\n",
    "#         binary_crossentropy, kullback_leibler_divergence, poisson, cosine_proximity\n",
    "# optimizer: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "# metrics: ?\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "train_size = 1\n",
    "test_size = 1\n",
    "time_size = 4000\n",
    "nb_residual_unit = 0\n",
    "lr = 0\n",
    "# model_name = 'vsp_%s_model.{epoch:03d}.h5' % model_type\n",
    "hyperparams_name = 'train_size{}.test_size{}.time_size{}.resunit{}.lr{}'.format(\n",
    "        train_size, test_size, time_size, nb_residual_unit, lr)\n",
    "fname_param = os.path.join('MODEL', '{}.best.h5'.format(hyperparams_name))\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, fname_param)\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_rmse', patience=2, mode='min')\n",
    "# model_checkpoint = ModelCheckpoint(\n",
    "#     fname_param, monitor='rmse', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "# print(\"\\nelapsed time (compiling model): %.3f seconds\\n\" %\n",
    "#         (time.time() - ts))\n",
    "\n",
    "# print('=' * 10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "callbacks = [checkpoint]#, lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(os.path.join('MODEL', '{}.best.h5'.format(hyperparams_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GraphDef cannot be larger than 2GB.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-49d9bc5b68cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nelapsed time (training cont): %.3f seconds\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 199\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1312\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1351\u001b[0m           \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m           graph_def, self._current_version = self._graph._as_graph_def(\n\u001b[0;32m-> 1353\u001b[0;31m               from_version=self._current_version, add_shapes=self._add_shapes)\n\u001b[0m\u001b[1;32m   1354\u001b[0m           \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyw/anaconda3/envs/tf-keras-py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3092\u001b[0m             \u001b[0mbytesize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3094\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GraphDef cannot be larger than 2GB.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3095\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_functions_to_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: GraphDef cannot be larger than 2GB."
     ]
    }
   ],
   "source": [
    "# print \"training model...\"\n",
    "ts = time.time()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=None,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          shuffle=True,\n",
    "          steps_per_epoch=1,\n",
    "          validation_steps=1,\n",
    "          callbacks=callbacks)\n",
    "print(\"\\nelapsed time (training cont): %.3f seconds\\n\" % (time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(fname_param)\n",
    "# from models import model_from_json\n",
    "\n",
    "json_string = model.to_json()\n",
    "open('model_json', 'w').write(json_string)\n",
    "#model = model_from_json(json_string)\n",
    "model.save_weights(os.path.join( save_dir,'MODEL', '{}.final.best.h5'.format(hyperparams_name)), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "ts = time.time()\n",
    "scores = model.evaluate(x_valid, y_valid, batch_size=y_valid.shape[0], verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "# print('Test score: %.6f rmse (norm): %.6f rmse (real): %.6f' %\n",
    "#         (score[0], score[1], score[1] * (mx - mn) / 2.))\n",
    "print(\"\\nelapsed time (eval cont): %.3f seconds\\n\" % (time.time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prdiction\n",
    "y_predict = model.predict(x_test, batch_size = 100, verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (tf-keras-py27)",
   "language": "python",
   "name": "tf-keras-py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
